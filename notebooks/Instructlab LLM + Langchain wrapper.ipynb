{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "020a7495-c7d2-4882-8d9e-f74b519a8973",
   "metadata": {},
   "source": [
    "# Run Instruct Lab trained model in Notebook - Langchain Implementation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1ddf88e-4da2-4da8-821e-52113f6bacd8",
   "metadata": {},
   "source": [
    "Run `ilab serve` in your terminal to serve your model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b79eab40-578c-41b2-99e2-459a6c61abf7",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## LangChain Custom LLM Wrapper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cc3413e8-d06a-4c56-8b3f-191e04b15eb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Any, Dict, Iterator, List, Optional\n",
    "import requests\n",
    "import json\n",
    "from langchain_core.callbacks.manager import CallbackManagerForLLMRun\n",
    "from langchain_core.language_models.llms import LLM\n",
    "from langchain_core.outputs import GenerationChunk\n",
    "\n",
    "class InstructLabLLM(LLM):\n",
    "    \"\"\"A custom chat model that communicates with an instructlab server.\n",
    "\n",
    "    Example:\n",
    "\n",
    "        .. code-block:: python\n",
    "\n",
    "            model = InstructLabLLM(\n",
    "                url=\"http://localhost:5000/your-endpoint\",\n",
    "                model_name=\"models/merlinite-7b-lab-Q4_K_M.gguf\",\n",
    "                system_message=\"You are a helpful assistant.\"\n",
    "            )\n",
    "            result = model.invoke([HumanMessage(content=\"hello\")])\n",
    "            result = model.batch([[HumanMessage(content=\"hello\")],\n",
    "                                 [HumanMessage(content=\"world\")]])\n",
    "    \"\"\"\n",
    "\n",
    "    url: str\n",
    "    model_name: str\n",
    "    system_message: str\n",
    "    \"\"\"The URL of the instructlab server, the model name, and the system message.\"\"\"\n",
    "\n",
    "    def _call(\n",
    "        self,\n",
    "        prompt: str,\n",
    "        stop: Optional[List[str]] = None,\n",
    "        run_manager: Optional[CallbackManagerForLLMRun] = None,\n",
    "        **kwargs: Any,\n",
    "    ) -> str:\n",
    "        \"\"\"Run the LLM on the given input.\n",
    "\n",
    "        Args:\n",
    "            prompt: The prompt to generate from.\n",
    "            stop: Stop words to use when generating. Model output is cut off at the\n",
    "                first occurrence of any of the stop substrings.\n",
    "            run_manager: Callback manager for the run.\n",
    "            **kwargs: Arbitrary additional keyword arguments. These are usually passed\n",
    "                to the model provider API call.\n",
    "\n",
    "        Returns:\n",
    "            The model output as a string.\n",
    "        \"\"\"\n",
    "        if stop is not None:\n",
    "            raise ValueError(\"stop kwargs are not permitted.\")\n",
    "\n",
    "        payload = {\n",
    "            \"model\": self.model_name,\n",
    "            \"messages\": [\n",
    "                {\"role\": \"system\", \"content\": self.system_message},\n",
    "                {\"role\": \"user\", \"content\": prompt}\n",
    "            ]\n",
    "        }\n",
    "        headers = {\n",
    "            \"Content-Type\": \"application/json\"\n",
    "        }\n",
    "\n",
    "        response = requests.post(self.url, data=json.dumps(payload), headers=headers)\n",
    "\n",
    "        if response.status_code == 200:\n",
    "            result = response.json()\n",
    "            return result['choices'][0]['message']['content']\n",
    "        else:\n",
    "            raise Exception(f\"Request failed with status code {response.status_code}\")\n",
    "\n",
    "    def _stream(\n",
    "        self,\n",
    "        prompt: str,\n",
    "        stop: Optional[List[str]] = None,\n",
    "        run_manager: Optional[CallbackManagerForLLMRun] = None,\n",
    "        **kwargs: Any,\n",
    "    ) -> Iterator[GenerationChunk]:\n",
    "        \"\"\"Stream the LLM on the given prompt.\n",
    "\n",
    "        This method should be overridden by subclasses that support streaming.\n",
    "\n",
    "        Args:\n",
    "            prompt: The prompt to generate from.\n",
    "            stop: Stop words to use when generating. Model output is cut off at the\n",
    "                first occurrence of any of these substrings.\n",
    "            run_manager: Callback manager for the run.\n",
    "            **kwargs: Arbitrary additional keyword arguments. These are usually passed\n",
    "                to the model provider API call.\n",
    "\n",
    "        Returns:\n",
    "            An iterator of GenerationChunks.\n",
    "        \"\"\"\n",
    "        payload = {\n",
    "            \"model\": self.model_name,\n",
    "            \"messages\": [\n",
    "                {\"role\": \"system\", \"content\": self.system_message},\n",
    "                {\"role\": \"user\", \"content\": prompt}\n",
    "            ]\n",
    "        }\n",
    "        headers = {\n",
    "        'accept': 'application/json',\n",
    "        'Authorization': 'Bearer test',\n",
    "        'Content-Type': 'application/json'\n",
    "        }\n",
    "\n",
    "        response = requests.post(self.url, data=json.dumps(payload), headers=headers, stream=True)\n",
    "\n",
    "        if response.status_code == 200:\n",
    "            for line in response.iter_lines():\n",
    "                if line:\n",
    "                    result = json.loads(line.decode('utf-8'))\n",
    "                    chunk = GenerationChunk(text=result.get(\"output\", \"\"))\n",
    "                    if run_manager:\n",
    "                        run_manager.on_llm_new_token(chunk.text, chunk=chunk)\n",
    "                    yield chunk\n",
    "        else:\n",
    "            raise Exception(f\"Request failed with status code {response.status_code}\")\n",
    "\n",
    "    @property\n",
    "    def _identifying_params(self) -> Dict[str, Any]:\n",
    "        \"\"\"Return a dictionary of identifying parameters.\"\"\"\n",
    "        return {\"url\": self.url, \"model_name\": self.model_name, \"system_message\": self.system_message}\n",
    "\n",
    "    @property\n",
    "    def _llm_type(self) -> str:\n",
    "        \"\"\"Get the type of language model used by this chat model. Used for logging purposes only.\"\"\"\n",
    "        return \"instructlab_llm\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62052022-8905-445e-abe2-6955731a18fe",
   "metadata": {},
   "source": [
    "## Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "7eb5babc-2979-444b-a8ab-d5f33b8e8533",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import PromptTemplate\n",
    "from langchain.chains.llm import LLMChain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1c808ef2-83cc-4ae7-b4e4-7d179430df3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replace with your actual server URL and model details\n",
    "llm_config = {\n",
    "    \"base_llm_config\": {\n",
    "        \"url\": \"http://127.0.0.1:8000/v1/chat/completions\",\n",
    "        \"model_name\": \"models/granite-7b-lab-Q4_K_M.gguf\",\n",
    "        \"system_message\": \"You are a helpful assistant.\"\n",
    "    },\n",
    "    \"trained_llm_config\": {\n",
    "        \"url\": \"http://127.0.0.1:1234/v1/chat/completions\",\n",
    "        \"model_name\": \"instructlab-merlinite-7b-lab-trained/instructlab-merlinite-7b-lab-Q4_K_M.gguf\",\n",
    "        \"system_message\": \"You are a helpful assistant.\"\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01a3a352-3a1f-4896-96fc-ec6dd498b475",
   "metadata": {},
   "source": [
    "## Initialize the custom Instruct Lab LLMs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "67c7931a-54df-456d-b8de-0ead0a154dfa",
   "metadata": {},
   "outputs": [],
   "source": [
    "base_llm = InstructLabLLM(\n",
    "    url=llm_config['base_llm_config']['url'], \n",
    "    model_name=llm_config['base_llm_config']['model_name'], \n",
    "    system_message=llm_config['base_llm_config']['system_message']\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "edf7f1ad-2f94-4138-af79-ef4be6e39e55",
   "metadata": {},
   "outputs": [],
   "source": [
    "trained_llm = InstructLabLLM(\n",
    "    url=llm_config['trained_llm_config']['url'], \n",
    "    model_name=llm_config['trained_llm_config']['model_name'], \n",
    "    system_message=llm_config['trained_llm_config']['system_message']\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adb2f3d5-f03e-4c36-88bc-752ef2c95f34",
   "metadata": {},
   "source": [
    "## Create a Prompt Template"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "5b0c02f9-d771-4153-b7b2-66ce9171f4da",
   "metadata": {},
   "outputs": [],
   "source": [
    "template = \"\"\"<|INSTRUCTION|>\n",
    "Act as a coding assistant. You are good in generating test cases for a given code snippet. Analyse the given code snipet and generate test cases for it.\n",
    "\n",
    "<|USER|>\n",
    "Code Snippet:\n",
    "{snippet}\n",
    "\n",
    "<|ASSISTANT|>\n",
    "Test Cases: \"\"\"\n",
    "\n",
    "prompt = PromptTemplate(input_variables=[\"snippet\"], template=template)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30c512c6-42e7-4a0d-b6e4-3268d579eef9",
   "metadata": {},
   "source": [
    "## Create Langchain wrapper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "6d54f806-1450-460e-974e-44f67a028839",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a LangChain with the InstructLab LLM and the prompt template\n",
    "base_llm_chain = LLMChain(prompt=prompt, llm=base_llm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "e75732cd-4e26-46dd-af31-d40fbcc495b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "trained_llm_chain = LLMChain(prompt=prompt, llm=trained_llm)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2aabce7-02cd-4eed-9c1f-28042edb0eb9",
   "metadata": {},
   "source": [
    "## Query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "e0618d8c-fa23-4168-b958-8406d164cff8",
   "metadata": {},
   "outputs": [],
   "source": [
    "snippet = \"\"\"\n",
    "def mult(x, y):\n",
    "    return x * y\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "06e2b9d3-9602-4e71-b65e-997045b85dac",
   "metadata": {},
   "outputs": [],
   "source": [
    "base_response = base_llm_chain.invoke(snippet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "c6cf8a7c-1690-42b0-a2f2-293a0c32d00c",
   "metadata": {},
   "outputs": [],
   "source": [
    "trained_response = trained_llm_chain.invoke(snippet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "f762193c-a840-4714-9897-ac5462642dd5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Base model response: \n",
      "----\n",
      "To thoroughly test the `mult` function, consider creating test cases that cover various scenarios, including positive and negative integers, zero, and floating-point numbers. Here are some test cases for your code snippet:\n",
      "\n",
      "1. Positive Integer Multiplication:\n",
      "   - Test with an integer value of 1: `mult(1, 2)` should return `3`.\n",
      "   - Test with an integer value of 2: `mult(2, 3)` should return `9`.\n",
      "   - Test with an integer value of 3: `mult(3, 4)` should return `12`.\n",
      "\n",
      "2. Negative Integer Multiplication:\n",
      "   - Test with a negative integer value of 1: `mult(-1, 2)` should return `-3`.\n",
      "   - Test with a negative integer value of 2: `mult(-2, 3)` should return `-7`.\n",
      "   - Test with a negative integer value of 3: `mult(-3, 4)` should return `-10`.\n",
      "\n",
      "3. Zero Multiplication:\n",
      "   - Test with zero: `mult(0, 2)` should return `0`.\n",
      "   - Test with zero: `mult(0, 3)` should return `0`.\n",
      "\n",
      "4. Floating-Point Number Multiplication:\n",
      "   - Test with a floating-point number value of 1.1: `mult(1.1, 2)` should return approximately `2.2`.\n",
      "   - Test with a floating-point number value of 1.0: `mult(1.0, 2)` should return `2.0`.\n",
      "\n",
      "5. Infinite Number Multiplication:\n",
      "   - Test with an infinite number (represented as `np.inf` in Python): `mult(np.inf, 2)` should raise a `ValueError`.\n",
      "   - Test with a negative infinite number (represented as `-np.inf` in Python): `mult(-np.inf, 2)` should raise a `ValueError`.\n",
      "\n",
      "These test cases cover a range of input values and scenarios that can help ensure the correctness and robustness of your `mult` function. Remember to execute these test cases in your preferred testing framework or environment.\n",
      "----\n"
     ]
    }
   ],
   "source": [
    "print(\"Base model response: \", \"----\", base_response['text'], \"----\", sep=\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "7cfbfc5e-1993-4421-b8eb-0609989e4b7b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "InstructLab trained model response: \n",
      "----\n",
      "import unittest\n",
      "\n",
      "class TestMult(unittest.TestCase):\n",
      "\n",
      "    def test_mult_positive_numbers(self):\n",
      "        self.assertEqual(mult(2, 3), 6)\n",
      "\n",
      "    def test_mult_negative_numbers(self):\n",
      "        self.assertEqual(mult(-2, -3), 6)\n",
      "\n",
      "    def test_mult_mixed_numbers(self):\n",
      "        self.assertEqual(mult(2, -3), -6)\n",
      "\n",
      "if __name__ == \"__main__\":\n",
      "    unittest.main()\n",
      "----\n"
     ]
    }
   ],
   "source": [
    "print(\"InstructLab trained model response: \", \"----\", trained_response['text'], \"----\", sep=\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c78557bf-7c95-42e8-b7ca-2b53c0057c6a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "langchain-watsonx",
   "language": "python",
   "name": "langchain-watsonx"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
